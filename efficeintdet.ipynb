{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T05:45:14.484380Z",
     "iopub.status.busy": "2024-07-01T05:45:14.484010Z",
     "iopub.status.idle": "2024-07-01T05:45:14.489149Z",
     "shell.execute_reply": "2024-07-01T05:45:14.488251Z",
     "shell.execute_reply.started": "2024-07-01T05:45:14.484351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tensorflow.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T05:45:17.647775Z",
     "iopub.status.busy": "2024-07-01T05:45:17.646825Z",
     "iopub.status.idle": "2024-07-01T05:46:04.964538Z",
     "shell.execute_reply": "2024-07-01T05:46:04.963462Z",
     "shell.execute_reply.started": "2024-07-01T05:45:17.647739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title\n",
    "import os\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "# Download source code.\n",
    "if \"efficientdet\" not in os.getcwd():\n",
    "  !git clone --depth 1 https://github.com/google/automl\n",
    "  os.chdir('automl/efficientdet')\n",
    "  sys.path.append('.')\n",
    "  !pip install -r requirements.txt\n",
    "  !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "else:\n",
    "  !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T05:46:32.827238Z",
     "iopub.status.busy": "2024-07-01T05:46:32.826704Z",
     "iopub.status.idle": "2024-07-01T05:46:42.249277Z",
     "shell.execute_reply": "2024-07-01T05:46:42.248317Z",
     "shell.execute_reply.started": "2024-07-01T05:46:32.827202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-01 05:46:33--  https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d7.tar.gz\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.195.207, 142.251.184.207, 172.217.214.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.195.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 413468440 (394M) [application/x-tar]\n",
      "Saving to: 'efficientdet-d7.tar.gz'\n",
      "\n",
      "efficientdet-d7.tar 100%[===================>] 394.31M   249MB/s    in 1.6s    \n",
      "\n",
      "2024-07-01 05:46:35 (249 MB/s) - 'efficientdet-d7.tar.gz' saved [413468440/413468440]\n",
      "\n",
      "Use model in /kaggle/working/automl/efficientdet/efficientdet-d7\n",
      "--2024-07-01 05:46:41--  https://user-images.githubusercontent.com/11736571/77320690-099af300-6d37-11ea-9d86-24f14dc2d540.png\n",
      "Resolving user-images.githubusercontent.com (user-images.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to user-images.githubusercontent.com (user-images.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4080549 (3.9M) [image/png]\n",
      "Saving to: 'img.png'\n",
      "\n",
      "img.png             100%[===================>]   3.89M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2024-07-01 05:46:42 (50.8 MB/s) - 'img.png' saved [4080549/4080549]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'efficientdet-d7'  #@param\n",
    "\n",
    "def download(m):\n",
    "  if m not in os.listdir():\n",
    "    !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/{m}.tar.gz\n",
    "    !tar zxf {m}.tar.gz\n",
    "  ckpt_path = os.path.join(os.getcwd(), m)\n",
    "  return ckpt_path\n",
    "\n",
    "# Download checkpoint.\n",
    "ckpt_path = download(MODEL)\n",
    "print('Use model in {}'.format(ckpt_path))\n",
    "\n",
    "# Prepare image and visualization settings.\n",
    "image_url =  'https://user-images.githubusercontent.com/11736571/77320690-099af300-6d37-11ea-9d86-24f14dc2d540.png'#@param\n",
    "image_name = 'img.png' #@param\n",
    "!wget {image_url} -O img.png\n",
    "import os\n",
    "img_path = os.path.join(os.getcwd(), 'img.png')\n",
    "\n",
    "min_score_thresh = 0.35  #@param\n",
    "max_boxes_to_draw = 200  #@param\n",
    "line_thickness =   2#@param\n",
    "\n",
    "import PIL\n",
    "# Get the largest of height/width and round to 128.\n",
    "image_size = max(PIL.Image.open(img_path).size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T05:46:50.017664Z",
     "iopub.status.busy": "2024-07-01T05:46:50.017183Z",
     "iopub.status.idle": "2024-07-01T05:46:55.440074Z",
     "shell.execute_reply": "2024-07-01T05:46:55.438849Z",
     "shell.execute_reply.started": "2024-07-01T05:46:50.017623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: automl/efficientdet/dataset: No such file or directory\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   905  100   905    0     0   4517      0 --:--:-- --:--:-- --:--:--  4525\n",
      "100 93.6M  100 93.6M    0     0  98.6M      0 --:--:-- --:--:-- --:--:--  159M\n",
      "Archive:  roboflow.zip\n",
      "  inflating: dataset/README.dataset.txt  \n",
      "  inflating: dataset/README.roboflow.txt  \n",
      "   creating: dataset/test/\n",
      " extracting: dataset/test/visual-pollutants.tfrecord  \n",
      "  inflating: dataset/test/visual-pollutants_label_map.pbtxt  \n",
      "   creating: dataset/train/\n",
      " extracting: dataset/train/visual-pollutants.tfrecord  \n",
      "  inflating: dataset/train/visual-pollutants_label_map.pbtxt  \n",
      "images_per_epoch = 195\n"
     ]
    }
   ],
   "source": [
    "# # Get pascal voc 2012 trainval data\n",
    "# import os\n",
    "# if 'VOCdevkit' not in os.listdir():\n",
    "#   !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "#   !tar xf VOCtrainval_06-Nov-2007.tar\n",
    "\n",
    "#   !mkdir tfrecord\n",
    "#   !python -m dataset.create_pascal_tfrecord  \\\n",
    "#     --data_dir=VOCdevkit --year=VOC2007  --output_path=tfrecord/pascal\n",
    "\n",
    "# # Pascal has 5717 train images with 100 shards epoch, here we use a single shard\n",
    "# # for demo, but users should use all shards pascal-*-of-00100.tfrecord.\n",
    "# file_pattern = 'pascal-00000-of-00100.tfrecord'  # @param\n",
    "# images_per_epoch = 57 * len(tf.io.gfile.glob('tfrecord/' + file_pattern))\n",
    "# images_per_epoch = images_per_epoch // 8 * 8  # round to 64.\n",
    "# print('images_per_epoch = {}'.format(images_per_epoch))\n",
    "\n",
    "import os\n",
    "!cd automl/efficientdet/dataset\n",
    "!curl -L \"https://app.roboflow.com/ds/H96YBtcnAw?key=4jWyU34taw\" > roboflow.zip\n",
    "!unzip roboflow.zip -d dataset\n",
    "!rm roboflow.zip\n",
    "\n",
    "# # Count the number of images\n",
    "# dataset_dir = 'dataset'\n",
    "# batch_size = 16\n",
    "# num_images = len([name for name in os.listdir(dataset_dir) if os.path.isfile(os.path.join(dataset_dir, name)) and name.endswith('.jpg')])\n",
    "\n",
    "# # Set images per epoch\n",
    "# images_per_epoch = (num_images // batch_size) * batch_size\n",
    "# print('Images per epoch:', images_per_epoch)\n",
    "# Since you know the total number of images\n",
    "total_images = 1566\n",
    "\n",
    "# Round to the nearest multiple of 64\n",
    "images_per_epoch = total_images //8\n",
    "\n",
    "print('images_per_epoch = {}'.format(images_per_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T05:48:07.732569Z",
     "iopub.status.busy": "2024-07-01T05:48:07.732195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# generating train tfrecord is large, so we skip the execution here.\n",
    "import os\n",
    "if MODEL not in os.listdir():\n",
    "  download(MODEL)\n",
    "\n",
    "\n",
    "!mkdir /tmp/model_dir/\n",
    "# key option: use --ckpt rather than --backbone_ckpt.\n",
    "# Run training script with specified arguments\n",
    "\n",
    "\n",
    "\n",
    "!python -m tf2.train --mode=traineval \\\n",
    "    --train_file_pattern=dataset/train/visual-pollutants.tfrecord \\\n",
    "    --val_file_pattern=dataset/test/visual-pollutants.tfrecord\\\n",
    "    --model_name={MODEL} \\\n",
    "    --model_dir=/tmp/model_dir/{MODEL}-finetune \\\n",
    "    --pretrained_ckpt={MODEL} \\\n",
    "    --batch_size=8 \\\n",
    "    --eval_samples={images_per_epoch}  \\\n",
    "    --num_examples_per_epoch={images_per_epoch}  --num_epochs=100  \\\n",
    "    --hparams=\"num_classes=5,moving_average_decay=0,mixed_precision=true\"\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
